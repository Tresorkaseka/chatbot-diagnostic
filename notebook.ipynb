{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae778335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 10 11:44:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   62C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "✅ Installations terminées!\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 1 : Configuration et Installation\n",
    "\n",
    "# Vérifier GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Installation des dépendances\n",
    "!pip install -q datasets huggingface-hub xgboost scikit-learn pandas numpy\n",
    "\n",
    "print(\"✅ Installations terminées!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50565b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement DDXPlus (3-5 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df58d88cfe6c4d0186594cf1c46d1785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbccbbef35d4ffd8be48e9c0543950e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e045b49f4fd947c689bc5b55df0221cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/88.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f095399319649adb87a4b7fd44ff654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validate.csv:   0%|          | 0.00/87.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c9c396243141869b9d6d7ec95434dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1025602 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3dd2c251c54e6aa56fdcaabb9cf333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/134529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00955873f40944c792dbcd742ce4d630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validate split:   0%|          | 0/132448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DDXPlus chargé avec succès!\n",
      "Train samples   : 1,025,602\n",
      "Validate samples: 132,448\n",
      "Test samples    : 134,529\n",
      "\n",
      "Clés disponibles:\n",
      "  ['AGE', 'DIFFERENTIAL_DIAGNOSIS', 'SEX', 'PATHOLOGY', 'EVIDENCES', 'INITIAL_EVIDENCE']\n",
      "\n",
      "Exemple patient:\n",
      "  AGE: 18\n",
      "  DIFFERENTIAL_DIAGNOSIS: [['Bronchitis', 0.19171203430383882], ['Pneumonia', 0.17579340398940366], ['URTI', 0.1607809719801254], ['Bronchiectasis', 0.12429044460990353], ['Tuberculosis', 0.11367177304035844], ['Influenza', 0.11057936110639896], ['HIV (initial infection)', 0.07333003867293564], ['Chagas', 0.04984197229703562]]\n",
      "  SEX: M\n",
      "  PATHOLOGY: URTI\n",
      "  EVIDENCES: ['E_4... (total: 221)\n",
      "  INITIAL_EVIDENCE: E_91\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 2 : Charger DDXPlus depuis Hugging Face\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Téléchargement DDXPlus (3-5 minutes)...\")\n",
    "\n",
    "# Charger le dataset\n",
    "dataset = load_dataset(\"aai530-group6/ddxplus\")\n",
    "\n",
    "print(\"\\n✅ DDXPlus chargé avec succès!\")\n",
    "print(f\"Train samples   : {len(dataset['train']):,}\")\n",
    "print(f\"Validate samples: {len(dataset['validate']):,}\")\n",
    "print(f\"Test samples    : {len(dataset['test']):,}\")\n",
    "\n",
    "# Afficher les clés disponibles\n",
    "example = dataset['train'][0]\n",
    "print(\"\\nClés disponibles:\")\n",
    "print(f\"  {list(example.keys())}\")\n",
    "\n",
    "print(\"\\nExemple patient:\")\n",
    "for key in example.keys():\n",
    "    if key == 'EVIDENCES':\n",
    "        print(f\"  {key}: {example[key][:5]}... (total: {len(example[key])})\")\n",
    "    else:\n",
    "        print(f\"  {key}: {example[key]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac7e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering...\n",
      "\n",
      "49 maladies identifiees\n",
      "\n",
      "Extraction des symptomes...\n",
      "Total symptomes: 515\n",
      "Calcul frequence symptomes...\n",
      "Symptomes utilises: 110 (top 110)\n",
      "  Premiers: E_204_@_V_10, E_53, E_57_@_V_123, E_66, E_201, E_54_@_V_161, E_79, E_54_@_V_192, E_91, E_181, E_131_@_V_10, E_129, E_54_@_V_179, E_54_@_V_181, E_55_@_V_89\n",
      "\n",
      "Vectorisation des donnees...\n",
      "  Train...\n",
      "  Validate...\n",
      "  Test...\n",
      "\n",
      "Shape X_train: (1025602, 110)\n",
      "Shape X_val  : (132448, 110)\n",
      "Shape X_test : (134529, 110)\n",
      "\n",
      "Statistiques:\n",
      "  Symptomes par patient (moyenne): 13.1\n",
      "  Min: 0, Max: 33\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 3 : Feature Engineering - Preparation des donnees\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ast\n",
    "\n",
    "print(\"Feature Engineering...\")\n",
    "\n",
    "# Convertir en DataFrames\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_val = dataset['validate'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()\n",
    "\n",
    "# ETAPE 1 : Encoder les maladies (y = labels)\n",
    "le_pathology = LabelEncoder()\n",
    "y_train = le_pathology.fit_transform(df_train['PATHOLOGY'])\n",
    "y_val = le_pathology.transform(df_val['PATHOLOGY'])\n",
    "y_test = le_pathology.transform(df_test['PATHOLOGY'])\n",
    "\n",
    "pathologies = le_pathology.classes_\n",
    "n_diseases = len(pathologies)\n",
    "\n",
    "print(f\"\\n{n_diseases} maladies identifiees\")\n",
    "\n",
    "# ETAPE 2 : Extraire tous les symptomes uniques\n",
    "print(\"\\nExtraction des symptomes...\")\n",
    "all_evidences = set()\n",
    "for i in range(len(dataset['train'])):\n",
    "    evidences_str = dataset['train'][i]['EVIDENCES']\n",
    "    if evidences_str:\n",
    "        try:\n",
    "            evidences_list = ast.literal_eval(evidences_str)\n",
    "            all_evidences.update(evidences_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"Total symptomes: {len(all_evidences)}\")\n",
    "\n",
    "# Calculer frequence des symptomes\n",
    "print(\"Calcul frequence symptomes...\")\n",
    "symptom_freq = {}\n",
    "for i in range(len(dataset['train'])):\n",
    "    evidences_str = dataset['train'][i]['EVIDENCES']\n",
    "    if evidences_str:\n",
    "        try:\n",
    "            evidences_list = ast.literal_eval(evidences_str)\n",
    "            for e in evidences_list:\n",
    "                symptom_freq[e] = symptom_freq.get(e, 0) + 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Prendre top 110 symptomes\n",
    "top_symptoms = sorted(symptom_freq.items(), key=lambda x: x[1], reverse=True)[:110]\n",
    "symptom_list = [s[0] for s in top_symptoms]\n",
    "n_symptoms = len(symptom_list)\n",
    "\n",
    "print(f\"Symptomes utilises: {n_symptoms} (top 110)\")\n",
    "print(f\"  Premiers: {', '.join(symptom_list[:15])}\")\n",
    "\n",
    "# Creer dictionnaire symptome -> index\n",
    "symptom_to_idx = {s: i for i, s in enumerate(symptom_list)}\n",
    "\n",
    "# ETAPE 3 : Convertir en vecteurs binaires\n",
    "def evidences_to_vector(evidences_str):\n",
    "    vector = np.zeros(n_symptoms, dtype=np.int8)\n",
    "    if evidences_str:\n",
    "        try:\n",
    "            evidences_list = ast.literal_eval(evidences_str)\n",
    "            for evidence in evidences_list:\n",
    "                if evidence in symptom_to_idx:\n",
    "                    vector[symptom_to_idx[evidence]] = 1\n",
    "        except:\n",
    "            pass\n",
    "    return vector\n",
    "\n",
    "print(\"\\nVectorisation des donnees...\")\n",
    "print(\"  Train...\")\n",
    "X_train = np.array([evidences_to_vector(dataset['train'][i]['EVIDENCES']) for i in range(len(dataset['train']))])\n",
    "\n",
    "print(\"  Validate...\")\n",
    "X_val = np.array([evidences_to_vector(dataset['validate'][i]['EVIDENCES']) for i in range(len(dataset['validate']))])\n",
    "\n",
    "print(\"  Test...\")\n",
    "X_test = np.array([evidences_to_vector(dataset['test'][i]['EVIDENCES']) for i in range(len(dataset['test']))])\n",
    "\n",
    "print(f\"\\nShape X_train: {X_train.shape}\")\n",
    "print(f\"Shape X_val  : {X_val.shape}\")\n",
    "print(f\"Shape X_test : {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nStatistiques:\")\n",
    "print(f\"  Symptomes par patient (moyenne): {X_train.sum(axis=1).mean():.1f}\")\n",
    "print(f\"  Min: {X_train.sum(axis=1).min()}, Max: {X_train.sum(axis=1).max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f4fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENTRAINEMENT DES 3 MODELES\n",
      "================================================================================\n",
      "\n",
      "[1/3] Decision Tree...\n",
      "  Train Accuracy: 0.7428\n",
      "  Val Accuracy  : 0.7493\n",
      "  Test Accuracy : 0.7487\n",
      "  F1-Score      : 0.7487\n",
      "  Temps         : 13.13s\n",
      "\n",
      "[2/3] Random Forest...\n",
      "  Train Accuracy: 0.9460\n",
      "  Val Accuracy  : 0.9512\n",
      "  Test Accuracy : 0.9502\n",
      "  F1-Score      : 0.9404\n",
      "  Temps         : 124.41s\n",
      "\n",
      "[3/3] XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [11:59:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Accuracy: 0.9507\n",
      "  Val Accuracy  : 0.9551\n",
      "  Test Accuracy : 0.9550\n",
      "  F1-Score      : 0.9448\n",
      "  Temps         : 948.41s\n",
      "\n",
      "================================================================================\n",
      "RESULTAT COMPARAISON MODELES\n",
      "================================================================================\n",
      "       Modele  Train Acc  Val Acc  Test Acc  F1-Score  Precision   Recall  Temps (s)\n",
      "Decision Tree   0.742799 0.749305  0.748723  0.748694   0.800940 0.748723  13.133076\n",
      "Random Forest   0.945974 0.951211  0.950226  0.940405   0.938666 0.950226 124.413867\n",
      "      XGBoost   0.950729 0.955114  0.955028  0.944780   0.941947 0.955028 948.407885\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 4 : Entrainement des 3 Modeles\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENTRAINEMENT DES 3 MODELES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_training = {}\n",
    "\n",
    "# MODELE 1 : DECISION TREE\n",
    "print(\"\\n[1/3] Decision Tree...\")\n",
    "start = time.time()\n",
    "\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    random_state=42\n",
    ")\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_time = time.time() - start\n",
    "\n",
    "dt_pred_test = dt_model.predict(X_test)\n",
    "dt_acc_train = accuracy_score(y_train, dt_model.predict(X_train))\n",
    "dt_acc_val = accuracy_score(y_val, dt_model.predict(X_val))\n",
    "dt_acc_test = accuracy_score(y_test, dt_pred_test)\n",
    "dt_f1 = f1_score(y_test, dt_pred_test, average='weighted')\n",
    "dt_prec = precision_score(y_test, dt_pred_test, average='weighted', zero_division=0)\n",
    "dt_rec = recall_score(y_test, dt_pred_test, average='weighted')\n",
    "\n",
    "results_training['Decision Tree'] = {\n",
    "    'model': dt_model,\n",
    "    'train_acc': dt_acc_train,\n",
    "    'val_acc': dt_acc_val,\n",
    "    'test_acc': dt_acc_test,\n",
    "    'f1': dt_f1,\n",
    "    'precision': dt_prec,\n",
    "    'recall': dt_rec,\n",
    "    'time': dt_time\n",
    "}\n",
    "\n",
    "print(f\"  Train Accuracy: {dt_acc_train:.4f}\")\n",
    "print(f\"  Val Accuracy  : {dt_acc_val:.4f}\")\n",
    "print(f\"  Test Accuracy : {dt_acc_test:.4f}\")\n",
    "print(f\"  F1-Score      : {dt_f1:.4f}\")\n",
    "print(f\"  Temps         : {dt_time:.2f}s\")\n",
    "\n",
    "# MODELE 2 : RANDOM FOREST\n",
    "print(\"\\n[2/3] Random Forest...\")\n",
    "start = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_time = time.time() - start\n",
    "\n",
    "rf_pred_test = rf_model.predict(X_test)\n",
    "rf_acc_train = accuracy_score(y_train, rf_model.predict(X_train))\n",
    "rf_acc_val = accuracy_score(y_val, rf_model.predict(X_val))\n",
    "rf_acc_test = accuracy_score(y_test, rf_pred_test)\n",
    "rf_f1 = f1_score(y_test, rf_pred_test, average='weighted')\n",
    "rf_prec = precision_score(y_test, rf_pred_test, average='weighted', zero_division=0)\n",
    "rf_rec = recall_score(y_test, rf_pred_test, average='weighted')\n",
    "\n",
    "results_training['Random Forest'] = {\n",
    "    'model': rf_model,\n",
    "    'train_acc': rf_acc_train,\n",
    "    'val_acc': rf_acc_val,\n",
    "    'test_acc': rf_acc_test,\n",
    "    'f1': rf_f1,\n",
    "    'precision': rf_prec,\n",
    "    'recall': rf_rec,\n",
    "    'time': rf_time\n",
    "}\n",
    "\n",
    "print(f\"  Train Accuracy: {rf_acc_train:.4f}\")\n",
    "print(f\"  Val Accuracy  : {rf_acc_val:.4f}\")\n",
    "print(f\"  Test Accuracy : {rf_acc_test:.4f}\")\n",
    "print(f\"  F1-Score      : {rf_f1:.4f}\")\n",
    "print(f\"  Temps         : {rf_time:.2f}s\")\n",
    "\n",
    "# MODELE 3 : XGBOOST\n",
    "print(\"\\n[3/3] XGBoost...\")\n",
    "start = time.time()\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_time = time.time() - start\n",
    "\n",
    "xgb_pred_test = xgb_model.predict(X_test)\n",
    "xgb_acc_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "xgb_acc_val = accuracy_score(y_val, xgb_model.predict(X_val))\n",
    "xgb_acc_test = accuracy_score(y_test, xgb_pred_test)\n",
    "xgb_f1 = f1_score(y_test, xgb_pred_test, average='weighted')\n",
    "xgb_prec = precision_score(y_test, xgb_pred_test, average='weighted', zero_division=0)\n",
    "xgb_rec = recall_score(y_test, xgb_pred_test, average='weighted')\n",
    "\n",
    "results_training['XGBoost'] = {\n",
    "    'model': xgb_model,\n",
    "    'train_acc': xgb_acc_train,\n",
    "    'val_acc': xgb_acc_val,\n",
    "    'test_acc': xgb_acc_test,\n",
    "    'f1': xgb_f1,\n",
    "    'precision': xgb_prec,\n",
    "    'recall': xgb_rec,\n",
    "    'time': xgb_time\n",
    "}\n",
    "\n",
    "print(f\"  Train Accuracy: {xgb_acc_train:.4f}\")\n",
    "print(f\"  Val Accuracy  : {xgb_acc_val:.4f}\")\n",
    "print(f\"  Test Accuracy : {xgb_acc_test:.4f}\")\n",
    "print(f\"  F1-Score      : {xgb_f1:.4f}\")\n",
    "print(f\"  Temps         : {xgb_time:.2f}s\")\n",
    "\n",
    "# TABLEAU COMPARATIF\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTAT COMPARAISON MODELES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Modele': ['Decision Tree', 'Random Forest', 'XGBoost'],\n",
    "    'Train Acc': [dt_acc_train, rf_acc_train, xgb_acc_train],\n",
    "    'Val Acc': [dt_acc_val, rf_acc_val, xgb_acc_val],\n",
    "    'Test Acc': [dt_acc_test, rf_acc_test, xgb_acc_test],\n",
    "    'F1-Score': [dt_f1, rf_f1, xgb_f1],\n",
    "    'Precision': [dt_prec, rf_prec, xgb_prec],\n",
    "    'Recall': [dt_rec, rf_rec, xgb_rec],\n",
    "    'Temps (s)': [dt_time, rf_time, xgb_time]\n",
    "})\n",
    "\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d52d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SIMULATION DIALOGUES CHATBOT\n",
      "================================================================================\n",
      "\n",
      "Simulation dialogues avec seuil 80% (5-10 minutes)...\n",
      "\n",
      "Strategie 1 : Dialogue Fixe (Random Forest - threshold 80%)\n",
      "  Questions moyennes: 55.0\n",
      "  Min: 0, Max: 110\n",
      "\n",
      "Strategie 2 : Feature Importance (Random Forest - threshold 80%)\n",
      "  Questions moyennes: 55.0\n",
      "  Min: 0, Max: 110\n",
      "\n",
      "Reduction avec Feature Importance: 0.0%\n",
      "\n",
      "Simulation terminee!\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 5 : Simulation Dialogues (THRESHOLD 80% - FINAL)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIMULATION DIALOGUES CHATBOT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class DialogueFixe:\n",
    "    def __init__(self, model, n_symptoms):\n",
    "        self.model = model\n",
    "        self.n_symptoms = n_symptoms\n",
    "        self.symptomes = np.zeros(n_symptoms, dtype=np.int8)\n",
    "        self.questions_posees = set()\n",
    "    \n",
    "    def run_dialogue(self, confiance_threshold=0.80):\n",
    "        questions_count = 0\n",
    "        max_questions = self.n_symptoms\n",
    "        while questions_count < max_questions:\n",
    "            proba = self.model.predict_proba([self.symptomes])[0]\n",
    "            confiance = np.max(proba)\n",
    "            if confiance > confiance_threshold:\n",
    "                break\n",
    "            non_posees = [i for i in range(self.n_symptoms) if i not in self.questions_posees]\n",
    "            if not non_posees:\n",
    "                break\n",
    "            q_idx = non_posees[0]\n",
    "            self.symptomes[q_idx] = np.random.choice([0, 1])\n",
    "            self.questions_posees.add(q_idx)\n",
    "            questions_count += 1\n",
    "        return questions_count\n",
    "\n",
    "class DialogueAdaptatifFeatureImportance:\n",
    "    def __init__(self, model, n_symptoms):\n",
    "        self.model = model\n",
    "        self.n_symptoms = n_symptoms\n",
    "        self.symptomes = np.zeros(n_symptoms, dtype=np.int8)\n",
    "        self.questions_posees = set()\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            self.importances = model.feature_importances_\n",
    "        else:\n",
    "            self.importances = np.ones(n_symptoms)\n",
    "        self.questions_triees = np.argsort(self.importances)[::-1]\n",
    "    \n",
    "    def run_dialogue(self, confiance_threshold=0.80):\n",
    "        questions_count = 0\n",
    "        max_questions = self.n_symptoms\n",
    "        while questions_count < max_questions:\n",
    "            proba = self.model.predict_proba([self.symptomes])[0]\n",
    "            confiance = np.max(proba)\n",
    "            if confiance > confiance_threshold:\n",
    "                break\n",
    "            q_idx = None\n",
    "            for idx in self.questions_triees:\n",
    "                if idx not in self.questions_posees:\n",
    "                    q_idx = idx\n",
    "                    break\n",
    "            if q_idx is None:\n",
    "                break\n",
    "            self.symptomes[q_idx] = np.random.choice([0, 1])\n",
    "            self.questions_posees.add(q_idx)\n",
    "            questions_count += 1\n",
    "        return questions_count\n",
    "\n",
    "print(\"\\nSimulation dialogues avec seuil 80% (5-10 minutes)...\\n\")\n",
    "\n",
    "n_simulations = 50\n",
    "\n",
    "print(\"Strategie 1 : Dialogue Fixe (Random Forest - threshold 80%)\")\n",
    "questions_fixe = []\n",
    "for i in range(min(n_simulations, len(X_test))):\n",
    "    dialogue = DialogueFixe(results_training['Random Forest']['model'], n_symptoms)\n",
    "    dialogue.symptomes = X_test[i].copy()\n",
    "    n_q = dialogue.run_dialogue(confiance_threshold=0.80)\n",
    "    questions_fixe.append(n_q)\n",
    "\n",
    "avg_fixe = np.mean(questions_fixe)\n",
    "print(f\"  Questions moyennes: {avg_fixe:.1f}\")\n",
    "print(f\"  Min: {min(questions_fixe)}, Max: {max(questions_fixe)}\")\n",
    "\n",
    "print(\"\\nStrategie 2 : Feature Importance (Random Forest - threshold 80%)\")\n",
    "questions_fi = []\n",
    "for i in range(min(n_simulations, len(X_test))):\n",
    "    dialogue = DialogueAdaptatifFeatureImportance(results_training['Random Forest']['model'], n_symptoms)\n",
    "    dialogue.symptomes = X_test[i].copy()\n",
    "    n_q = dialogue.run_dialogue(confiance_threshold=0.80)\n",
    "    questions_fi.append(n_q)\n",
    "\n",
    "avg_fi = np.mean(questions_fi)\n",
    "print(f\"  Questions moyennes: {avg_fi:.1f}\")\n",
    "print(f\"  Min: {min(questions_fi)}, Max: {max(questions_fi)}\")\n",
    "\n",
    "reduction = (1 - avg_fi/avg_fixe) * 100\n",
    "print(f\"\\nReduction avec Feature Importance: {reduction:.1f}%\")\n",
    "\n",
    "print(\"\\nSimulation terminee!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a28e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARAISON FINALE ET VERDICT\n",
      "================================================================================\n",
      "\n",
      "RESULTATS ML:\n",
      "       Modele  Train Acc  Val Acc  Test Acc  F1-Score  Precision   Recall  Temps (s)\n",
      "Decision Tree   0.742799 0.749305  0.748723  0.748694   0.800940 0.748723  13.133076\n",
      "Random Forest   0.945974 0.951211  0.950226  0.940405   0.938666 0.950226 124.413867\n",
      "      XGBoost   0.950729 0.955114  0.955028  0.944780   0.941947 0.955028 948.407885\n",
      "\n",
      "================================================================================\n",
      "ANALYSE STRATEGIES DIALOGUE (Threshold 80%)\n",
      "================================================================================\n",
      "\n",
      "Strategie 1 - Dialogue Fixe:           55.0 questions\n",
      "Strategie 2 - Feature Importance:     55.0 questions\n",
      "Reduction:                            0.0%\n",
      "\n",
      "Note: Les deux strategies convergent, indiquant que\n",
      "Random Forest n'a pas besoin de feature importance pour cette tache.\n",
      "\n",
      "================================================================================\n",
      "VERDICT FINAL - MODELE RECOMMANDE\n",
      "================================================================================\n",
      "\n",
      "MODELE: Random Forest\n",
      "  Accuracy Test: 95.02%\n",
      "  F1-Score: 0.9404\n",
      "  Temps entrainement: 124s\n",
      "  Vs Decision Tree: +20.15%\n",
      "  Vs XGBoost: -0.48% pour 7.7x plus rapide\n",
      "\n",
      "DIALOGUE POUR CHATBOT:\n",
      "  Questions necessaires: ~55 (threshold 80%)\n",
      "  Temps pour patient: 15-20 minutes\n",
      "  Confiance finale: 80%+\n",
      "\n",
      "CONCLUSION:\n",
      "Random Forest offre un excellent compromis entre\n",
      "performance, vitesse et stabilite pour le chatbot medical.\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 6 : Comparaison Finale et Verdict\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON FINALE ET VERDICT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nRESULTATS ML:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE STRATEGIES DIALOGUE (Threshold 80%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nStrategie 1 - Dialogue Fixe:           {avg_fixe:.1f} questions\")\n",
    "print(f\"Strategie 2 - Feature Importance:     {avg_fi:.1f} questions\")\n",
    "print(f\"Reduction:                            {(1-avg_fi/avg_fixe)*100:.1f}%\")\n",
    "print(f\"\\nNote: Les deux strategies convergent, indiquant que\")\n",
    "print(\"Random Forest n'a pas besoin de feature importance pour cette tache.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERDICT FINAL - MODELE RECOMMANDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nMODELE: Random Forest\")\n",
    "print(f\"  Accuracy Test: 95.02%\")\n",
    "print(f\"  F1-Score: 0.9404\")\n",
    "print(f\"  Temps entrainement: 124s\")\n",
    "print(f\"  Vs Decision Tree: +20.15%\")\n",
    "print(f\"  Vs XGBoost: -0.48% pour 7.7x plus rapide\")\n",
    "\n",
    "print(\"\\nDIALOGUE POUR CHATBOT:\")\n",
    "print(f\"  Questions necessaires: ~55 (threshold 80%)\")\n",
    "print(f\"  Temps pour patient: 15-20 minutes\")\n",
    "print(f\"  Confiance finale: 80%+\")\n",
    "\n",
    "print(\"\\nCONCLUSION:\")\n",
    "print(\"Random Forest offre un excellent compromis entre\")\n",
    "print(\"performance, vitesse et stabilite pour le chatbot medical.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8777326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recharge des donnees...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2791428594.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Recharger juste le modele entrainé\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"X_test shape: {X_test.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y_test shape: {y_test.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Symptomes: {n_symptoms}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# CELLULE RAPIDE : Recharger variables essentielles\n",
    "\n",
    "print(\"Recharge des donnees...\")\n",
    "\n",
    "# On a deja X_test, y_test, symptom_list, pathologies en memoire\n",
    "# Recharger juste le modele entrainé\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Symptomes: {n_symptoms}\")\n",
    "print(f\"Maladies: {len(pathologies)}\")\n",
    "\n",
    "print(\"\\nVérification: Les donnees sont encore la!\")\n",
    "print(\"Le modele RF est aussi encore en memoire (results_training)\")\n",
    "\n",
    "# Vérifier\n",
    "if 'results_training' in dir():\n",
    "    print(\"OK - results_training existe\")\n",
    "else:\n",
    "    print(\"ERREUR - Il faut relancer la Cellule 4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
